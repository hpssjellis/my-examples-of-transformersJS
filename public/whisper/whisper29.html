<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Whisper-base Live Local Transcription</title>
  <script type="module">
    import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.2.2';

    window.loadModel = loadModel;
    window.startRecording = startRecording;
    window.stopRecording = stopRecording;

    let myASR;
    let myAudioContext;
    let myWorkletNode;
    let myAudioBuffer = [];
    const mySampleRate = 16000;
    const myChunkDuration = 2; // seconds

    async function loadModel() {
      myASR = await pipeline("automatic-speech-recognition", "Xenova/whisper-small.en");     //Xenova/whisper-base is accurate but slow. Try: .en for english
                                                                                         //Xenova/whisper-tiny (Fastest, but lower accuracy)
                                                                                         //Xenova/whisper-small (Good speed/accuracy balance)
      document.getElementById('loadModelButton').disabled = true;
      document.getElementById('startButton').disabled = false;
      console.log("Model loaded.");
      startRecording() 
    }

    async function startRecording() {
      document.getElementById('transcription').innerText = "";
      document.getElementById('startButton').disabled = true;
      document.getElementById('stopButton').disabled = false;
     // const myLanguage = document.getElementById('myLanguageSelect').value;
      myAudioContext = new AudioContext({ sampleRate: mySampleRate });

      await myAudioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
        class MyAudioProcessor extends AudioWorkletProcessor {
          process(inputs) {
            if (inputs[0].length > 0) {
              this.port.postMessage(inputs[0][0]);
            }
            return true;
          }
        }
        registerProcessor('my-audio-processor', MyAudioProcessor);
      `], { type: "application/javascript" })));

      myWorkletNode = new AudioWorkletNode(myAudioContext, 'my-audio-processor');
      myWorkletNode.port.onmessage = async (event) => {
        myAudioBuffer.push(...event.data);
        if (myAudioBuffer.length >= mySampleRate * myChunkDuration) {
          let myChunk = myAudioBuffer.slice(0, mySampleRate * myChunkDuration);
          myAudioBuffer = myAudioBuffer.slice(mySampleRate * myChunkDuration);          
          
        //  const myResult = await myASR(new Float32Array(myChunk), { language: myLanguage });
          const myResult = await myASR(new Float32Array(myChunk));
          let myText = myResult.text.trim();  // just to check for special codes.
         // console.log(myText)
          //var regExp = /\(([^)]+)\)/;
          const regExp= /[\(\{[][^()\{}]*[\)\}\]]/g
          if (regExp.exec(myText)){   // check for anything in brackets and only show in the console
            console.log(myText)
            if (myText == '[BLANK_AUDIO]' ){
              console.log('linefeed')
              document.getElementById('transcription').innerText += '\r\n'
            }
          } else {    // all good if no brackets then print the output to the webpage
              document.getElementById('transcription').innerText +=  myResult.text;
          }
          
        }
      };

      const myStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mySource = myAudioContext.createMediaStreamSource(myStream);
      mySource.connect(myWorkletNode);

      console.log("Live transcription started...");
    }

    function stopRecording() {
      document.getElementById('startButton').disabled = false;
      document.getElementById('stopButton').disabled = true;
      myWorkletNode.disconnect();
      myAudioContext.close();
      console.log("Recording stopped.");
    }
  </script>
</head>
<body>
  <h1>Live Whisper-small.en Local Transcription</h1>
  Note1: Unlike my other code, this one is having CORS issues when loading from a local computer, works fine online and then works completely client-side, just does not load clientside. 
  Working on it <br>
  Note2:  Based on the <a href="https://huggingface.co/Xenova/whisper-tiny">whisper-tiny</a>, <a href="https://huggingface.co/Xenova/whisper-small">whisper-small</a> and
  <a href="https://huggingface.co/Xenova/whisper-base">whisper-base</a> whisper models by Xenova on HuggingFace. Add ".en" for an English specific model<br><br>
  <input type="button" id="loadModelButton" value="Load Model" onclick="loadModel()">
  <input type="button" id="startButton" value="Start Live Transcription" onclick="startRecording()" disabled>
  <input type="button" id="stopButton" value="Stop" onclick="stopRecording()" disabled><br>

  <!--
  <select id="myLanguageSelect">
    <option value="ar">Arabic</option>
    <option value="zh">Chinese</option>
    <option value="en" selected>English</option>
    <option value="fr">French</option>
    <option value="de">German</option>
    <option value="it">Italian</option>
    <option value="ja">Japanese</option>
    <option value="ru">Russian</option>
    <option value="es">Spanish</option>
  </select>
 -->
  <h2>Transcription:</h2>
  <div id="transcription">...</div>


<br><br><br><br>
<hr>Use at your own risk, by <a href="https://www.linkedin.com/in/jeremy-ellis-4237a9bb/">Jeremy Ellis LinkedIn</a><br> 
Github index at <a href="https://hpssjellis.github.io/my-examples-of-ai-agents/public/index.html">hpssjellis.github.io/my-examples-of-ai-agents/public/index.html</a><br>
My transformersjs github where the work was done <a href="https://github.com/hpssjellis/my-examples-of-transformersJS">https://github.com/hpssjellis/my-examples-of-transformersJS</a><br>
My Github <a href="https://github.com/hpssjellis">Profile</a><br>

  
</body>
</html>
