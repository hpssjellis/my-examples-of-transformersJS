<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Whisper-base Live Transcription</title>
  <script type="module">
    // import { pipeline, read_audio } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers';   // use to test if latest works

    import { pipeline, read_audio } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.2.2';

    window.startRecording = startRecording;
    window.stopRecording = stopRecording;
    window.loadModel = loadModel;

    let asrPipeline;
    let audioContext;
    let mediaStream;
    let processorNode;
    let audioBuffer = [];
    const sampleRate = 8000;   //16000;
    const chunkDuration = 1; // seconds

    // Load Whisper model
    async function loadModel() {
      asrPipeline = await pipeline("automatic-speech-recognition", "Xenova/whisper-base");
      document.getElementById('loadModelButton').disabled = true;
      document.getElementById('startButton').disabled = false;
      console.log("Model loaded.");
      startRecording()   
    }


async function startRecording() {
  
  let myLanguage = document.getElementById('myLanguageSelect').value;
  document.getElementById('startButton').disabled = true;
  document.getElementById('stopButton').disabled = false;

  audioContext = new AudioContext({ sampleRate: sampleRate });
  mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const source = audioContext.createMediaStreamSource(mediaStream);

  // Create a ScriptProcessorNode for real-time processing with buffer size of 16384  or 4096
  processorNode = audioContext.createScriptProcessor(16384, 1, 1);
  source.connect(processorNode);
  processorNode.connect(audioContext.destination);

  processorNode.onaudioprocess = async (event) => {
    const audioData = event.inputBuffer.getChannelData(0);
    
//console.log("Buffer length before:", audioBuffer.length);
    audioBuffer.push(...audioData);

    if (audioBuffer.length >= sampleRate * chunkDuration) {
      
      console.log("Buffer length before:", audioBuffer.length);
      const chunk = audioBuffer.slice(0, sampleRate * chunkDuration);
      let audioRemainder = audioBuffer.slice(sampleRate * chunkDuration);
      
console.log("Processing chunk of size:", chunk.length);
console.log("Remaining buffer after slice:",  audioRemainder.length);

            // Zero out the circular buffer after processing
      audioBuffer.fill(0);
      
      console.log("Buffer after zero:",  audioBuffer.length);
      audioBuffer.push(audioRemainder);
      console.log("Buffer after remainder",  audioBuffer.length);

      // Convert chunk to Float32Array
      const float32Chunk = new Float32Array(chunk);

      const result = await asrPipeline(float32Chunk, { language: myLanguage });
      console.log(result);
      const myTextTrimmed = result.text.trim(); // remove spaces at the front or back
      if (myTextTrimmed == '[BLANK_AUDIO]' || myTextTrimmed == '[MUSIC]' || myTextTrimmed == '[BEEP]' || myTextTrimmed == '[COUGH]' || myTextTrimmed == '[INAUDIOBLE]') {
        // do nothing
      } else {
        document.getElementById('transcription').innerText += result.text;
      }


    }
  };

  console.log("Live transcription started...");
}







      
    // Stop recording
    function stopRecording() {
      document.getElementById('startButton').disabled = false;
      document.getElementById('stopButton').disabled = true;

      processorNode.disconnect();
      mediaStream.getTracks().forEach(track => track.stop());
      audioContext.close();

      console.log("Recording stopped. How to stop the processor?");
    }
  </script>
</head>
<body>
  <h1>Live Whisper-base Transcription</h1>
  <button id="loadModelButton" onclick="loadModel()">Load Model</button>
  <button id="startButton" onclick="startRecording()" disabled>Start Live Transcription</button>
  <button id="stopButton" onclick="stopRecording()" disabled>Stop</button><br>
  
  <select id="myLanguageSelect">
    <option value="ar">Arabic</option>
    <option value="zh">Chinese</option>
    <option value="en" selected>English</option>
    <option value="fr">French</option>
    <option value="de">German</option>
    <option value="it">Italian</option>
    <option value="ja">Japanese</option>
    <option value="ru">Russian</option>
    <option value="es">Spanish</option>
  </select>

  <h2>Transcription:</h2>
  <p id="transcription">...</p>
</body>
</html>
